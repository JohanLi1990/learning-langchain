{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50f5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# print(os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "#  connection string: postgresql+psycopg://langchain:langchain@localhost:6024/langchain\n",
    "PG_CONNECT_STRING = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc829d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7e6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('../../test.txt').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "chunks=text_splitter.split_documents(raw_documents)\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f3d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(chunks, embeddings_model, connection=PG_CONNECT_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30594bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e83da7bd-958f-46c0-9b75-0e7ecd06529a', metadata={'source': '../../test.txt'}, page_content='V.'),\n",
       " Document(id='3fd54062-2423-479f-bc11-e169f8cf3d4f', metadata={'source': '../../test.txt'}, page_content='V.'),\n",
       " Document(id='d62d25f3-9d6d-418e-bcc9-ce5c1c56904e', metadata={'source': '../../test.txt'}, page_content='V.'),\n",
       " Document(id='7b2c795d-ce0e-43bc-b47b-f9aaf435d858', metadata={'source': '../../test.txt'}, page_content='V.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"query\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21597d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eedb17e0-5c36-4569-86a4-e29ae0ce91b6',\n",
       " '1dd65ed6-71a8-4247-a644-7c9dcdce8c9e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"there are cats in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animal\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"ducks are also found in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animals\"}\n",
    "        )\n",
    "    ],\n",
    "    ids = ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a8b10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.delete([ids[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a157c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "\n",
    "collection_name =\"my_docs\"\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "namespace = \"my_docs_namespace\"\n",
    "\t\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=PG_CONNECT_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\t\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,\n",
    "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb72fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema if it doesn't exist\n",
    "record_manager.create_schema()\n",
    "\t\n",
    "# Create documents\n",
    "docs = [\n",
    "    Document(page_content='there are cats in the pond', metadata={\n",
    "        \"id\": 1, \"source\": \"cats.txt\"}),\n",
    "    Document(page_content='ducks are also found in the pond', metadata={\n",
    "        \"id\": 2, \"source\": \"ducks.txt\"}),\n",
    "]\n",
    "\t\n",
    "# Index the documents\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",  # prevent duplicate documents\n",
    "    source_id_key=\"source\",  # use the source field as the source_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae19f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 1: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index attempt 1:\", index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babf589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 2: {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# second time you attempt to index, it will not add the documents again\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "print(\"Index attempt 2:\", index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c928703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 3: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "# If we mutate a document, the new version will be written and all old \n",
    "# versions sharing the same source will be deleted.\n",
    "\t\n",
    "docs[0].page_content = \"I just modified this document!\"\n",
    "\t\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 3:\", index_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470cd34",
   "metadata": {},
   "source": [
    "## MultiVectorRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9493e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb58ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of loaded docs:  624212\n"
     ]
    }
   ],
   "source": [
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"summaries\"\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# Load the document\n",
    "loader = TextLoader(\"../../test.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"length of loaded docs: \", len(docs[0].page_content))\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# The rest of your code remains the same, starting from:\n",
    "prompt_text = \"Summarize the following document:\\n\\n{doc}\"\n",
    "\t\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "summarize_chain = {\n",
    "    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\t\n",
    "# batch the chain across the chunks\n",
    "summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ec9700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b002a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1 explores the rich cultural, social, and political aspects of life in ancient Greece, focusing on the importance of the polis, or city-state, in shaping Greek society. It discusses the communal living, intellectual pursuits, and artistic innovations that characterized ancient Greek civilization, as well as the social structure, education, religion, economy, and contributions to art and architecture that defined this remarkable society.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b0ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\t\n",
    "# indexing the summaries in our vector store, whilst retaining the original \n",
    "# documents in our document store:\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\t\n",
    "# Changed from summaries to chunks since we need same length as docs\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\t\n",
    "# Each summary is linked to the original document by the doc_id\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\t\n",
    "# Add the document summaries to the vector store for similarity search\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\t\n",
    "# Store the original documents in the document store, linked to their summaries \n",
    "# via doc_ids\n",
    "# This allows us to first search summaries efficiently, then fetch the full \n",
    "# docs when needed\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\t\n",
    "# vector store retrieves the summaries\n",
    "sub_docs = retriever.vectorstore.similarity_search(\n",
    "    \"chapter on philosophy\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfa349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b3d44e76-97d0-4570-9f8a-419ac1696ea9', metadata={'doc_id': '6527dddc-85ab-4239-8701-ca05dfbdd81d'}, page_content=\"This chapter discusses Plato, his teachers, and his times. It explores the evolution of philosophy and the high reputation of Plato's philosophy despite its extravagances. It also delves into the dialectic method used by Plato, his views on divine inspiration, and his criticism of Socratic ethics. The chapter also touches on Plato's relation to the Sophists and his philosophical teachings as embodied in his dialogues.\"),\n",
       " Document(id='56a420d8-140d-43a1-9de5-57c18337a183', metadata={'doc_id': 'b87fe0db-5e07-4c07-b15a-8cdbe6248e24'}, page_content='Chapter I discusses the early Greek thought, highlighting the strength and universality of the Greek intellect, the specialization of individual genius, and the pervading sense of harmony and union. It also explores the circumstances that shaped the intellectual character of the Greeks, emphasizing that philosophy was a natural product of the Greek mind. The chapter discusses how speculation was initially limited to the external world and the important results achieved by early Greek thinkers. It also addresses the conception of a cosmos that first made science possible and disproves the alleged influence of Oriental ideas on Greek thought.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b55ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas the retriever will return the larger source document chunks:\n",
    "retrieved_docs = retriever.invoke(\"chapter on philosophy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2282b2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../../test.txt'}, page_content='CHAPTER IV.\\n\\nPLATO; HIS TEACHERS AND HIS TIMES    pages 171-213\\n\\nI. New meaning given to systems of philosophy by the method of\\nevolution, 171—Extravagances of which Plato’s philosophy seems to be\\nmade up, 172—The high reputation which it, nevertheless, continues\\nto enjoy, 174—Distinction between speculative tendencies and the\\nsystematic form under which they are transmitted, 174—Genuineness\\nof the Platonic Dialogues, 175—Their chronological order, 177—They\\nembody the substance of Plato’s philosophical teaching, 177.\\n\\nII. Wider application given to the dialectic method by Plato, 179—He\\ngoes back to the initial doubt of Socrates, 180—To what extent\\nhe shared in the religious reaction of his time, 181—He places\\ndemonstrative reasoning above divine inspiration, 182—His criticism\\nof the Socratic ethics, 183—Exceptional character of the _Crito_\\naccounted for, 184—Traces of Sophistic influence, 185—General\\nrelation of Plato to the Sophists, 186—Egoistic hedonism of the\\n_Protagoras_, 188.'),\n",
       " Document(metadata={'source': '../../test.txt'}, page_content='CHAPTER I.\\n\\nEARLY GREEK THOUGHT    pages 1-52\\n\\nI. Strength and universality of the Greek intellect, 1—Specialisation\\nof individual genius, 2—Pervading sense of harmony and union,\\n3—Circumstances by which the intellectual character of the Greeks\\nwas determined, 3—Philosophy a natural product of the Greek mind,\\n4—Speculation at first limited to the external world, 4—Important\\nresults achieved by the early Greek thinkers, 5—Their conception of\\na cosmos first made science possible, 6—The alleged influence of\\nOriental ideas disproved, 6.'),\n",
       " Document(metadata={'source': '../../test.txt'}, page_content='CHAPTER VI.\\n\\nCHARACTERISTICS OF ARISTOTLE.\\n\\n\\nI.'),\n",
       " Document(metadata={'source': '../../test.txt'}, page_content='The title of this chapter may have seemed to promise more than a\\ncasual mention of the thinker in whom Greek Humanism attained its\\nloftiest and purest expression. But in history, no less than in life,\\nSocrates must ever stand apart from the Sophists. Beyond and above all\\nspecialities of teaching, the transcendent dignity of a character which\\npersonified philosophy itself demands a separate treatment. Readers who\\nhave followed us thus far may feel interested in an attempt to throw\\nsome new light on one who was a riddle to his contemporaries, and has\\nremained a riddle to after-ages.\\n\\nFOOTNOTES:')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f6aaaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyang/Git/learning-langchain/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/home/chenyang/Git/learning-langchain/.venv/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# RAGatouille is a library that makes it simple to use ColBERT\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#! pip install -U ragatouille\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m      5\u001b[39m RAG = RAGPretrainedModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mcolbert-ir/colbertv2.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/ragatouille/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.0.8post4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGPretrainedModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGTrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGTrainer\n\u001b[32m      5\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mRAGPretrainedModel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRAGTrainer\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/ragatouille/RAGPretrainedModel.py:14\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llama_index_sentence_splitter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     RAGatouilleLangChainCompressor,\n\u001b[32m     12\u001b[39m     RAGatouilleLangChainRetriever,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERT, LateInteractionModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRAGPretrainedModel\u001b[39;00m:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    Wrapper class for a pretrained RAG late-interaction model, and all the associated utilities.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    Allows you to load a pretrained model from disk or from the hub, build or query an index.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/ragatouille/models/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LateInteractionModel\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolbert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERT\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mLateInteractionModel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mColBERT\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/ragatouille/models/colbert.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrsly\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERTConfig, Run, RunConfig\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/colbert/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Indexer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msearcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Searcher\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/colbert/trainer.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlauncher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Launcher\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERTConfig, RunConfig\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTrainer\u001b[39;00m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, triples, queries, collection, config=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/learning-langchain/.venv/lib/python3.12/site-packages/colbert/training/training.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdamW, get_linear_schedule_with_warmup\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERTConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolbert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrerank_batcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RerankBatcher\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (/home/chenyang/Git/learning-langchain/.venv/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# RAGatouille is a library that makes it simple to use ColBERT\n",
    "#! pip install -U ragatouille\n",
    "\n",
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "## Create an index\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")\n",
    "\n",
    "#query\n",
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results\n",
    "\n",
    "#utilize langchain retriever\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5856d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
